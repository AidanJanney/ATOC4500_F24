{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Code for running 3DVAR with the Lorenz 63 model.\n",
    "#### Code developed by Greg Hakim, Ryan Torn, Aneesh Subramanian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from numpy.linalg import inv\n",
    "import netCDF4 as nc\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "import lorenz63_model as lor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data assimilation experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assim_len = 1.0  #  Time between observations\n",
    "fcst_len  = 2.0  #  Forecast length\n",
    "nassim    = 200  #  Number of assimilation times\n",
    "alpha     = 4.e-3  # Alpha control (how fast you try to converge to analysis)\n",
    "\n",
    "H = np.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])  #  observation operator\n",
    "#H = np.array([[1., 0., 0. ]])  #  observation operator for single observation\n",
    "nobs = len(H[:,0])\n",
    "R = np.eye(nobs) * 1.0e-2  #  observation error\n",
    "#R = np.array([[1.0e-2]])  #  observation error for single observation\n",
    "\n",
    "time1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "bfile = nc.Dataset('L63_B.nc')\n",
    "invB = inv(bfile.variables['B_matrix'][:,:])\n",
    "invR = inv(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = np.empty(3)\n",
    "xf = np.empty(3)\n",
    "xe = np.empty(3)\n",
    "\n",
    "yobs  = np.empty(nobs)\n",
    "innov = np.empty(nobs)\n",
    "\n",
    "xaerr = np.empty((nassim, 3))\n",
    "xberr = np.empty((nassim, 3))\n",
    "xferr = np.empty((nassim, 3))\n",
    "\n",
    "Jfin = np.empty(nassim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IC for truth taken from last time (column vector):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.array(lor.advance(10., 20., 30., 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate initial state by perturbing true state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa = np.empty(3)\n",
    "xa = xt[:] + np.random.normal(0, 0.1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(nassim):\n",
    "\n",
    "  #  Advance analysis to next assimilation time\n",
    "  xb[0], xb[1], xb[2] = lor.advance(xa[0], xa[1], xa[2], assim_len)\n",
    "\n",
    "  #  Advance the truth, compute observations at the next time\n",
    "  xt[0], xt[1], xt[2] = lor.advance(xt[0], xt[1], xt[2], assim_len)\n",
    "  yobs[:] = np.matmul(H,xt) + np.random.normal(0, np.diag(np.sqrt(R)), nobs)\n",
    "\n",
    "  xa[:] = xb[:]\n",
    "  niters = 0\n",
    "  maxiter = 100\n",
    "  Jold = 1.0e6\n",
    "  J = 0.\n",
    "  while abs(Jold - J) > 1.0e-5:\n",
    "\n",
    "    Jold = J\n",
    "\n",
    "    #  Compute innovation, background and observation cost function\n",
    "    innov[:] = yobs[:] - np.matmul(H,xa)\n",
    "    Jb = 2.0 * np.matmul(np.matmul(np.transpose(xa - xb), invB), xa - xb)\n",
    "    J0 = 2.0 * np.matmul(np.matmul(np.transpose(innov), invR), innov)\n",
    "    J = Jb + J0\n",
    "\n",
    "    print('   cost function = ',J,\", Error: \",np.sqrt(np.sum((xa[:]-xt[:])**2)))\n",
    "\n",
    "    #  Compute the gradient in the cost function\n",
    "    gJ = 2.0 * np.matmul(invB,xa - xb) - 2.0 * np.matmul(np.matmul(np.transpose(H),invR),innov)\n",
    "\n",
    "    #  Compute the new state vector based on cost function gradient\n",
    "    if niters == 0:\n",
    "      xa[:] = xa[:] - alpha*gJ[:]\n",
    "      cgJo = gJ[:]\n",
    "    else:\n",
    "      beta = np.matmul(np.transpose(gJ),gJ) / np.matmul(np.transpose(gJo),gJo)\n",
    "      cgJ = gJ[:] + beta*cgJo[:]\n",
    "      xa[:] = xa[:] - alpha*cgJ[:]\n",
    "      cgJo = cgJ[:]\n",
    "\n",
    "    gJo = gJ[:]\n",
    "\n",
    "    niters = niters + 1\n",
    "\n",
    "  print('final cost = ', J, ' after ', niters, ' iterations')\n",
    "\n",
    "  Jfin[t] = J\n",
    "\n",
    "  #  Compute analysis and background forecast error\n",
    "  xberr[t,:] = xb[:] - xt[:]\n",
    "  xaerr[t,:] = xa[:] - xt[:]\n",
    "\n",
    "  # compute forecast and error\n",
    "  xf[0], xf[1], xf[2] = lor.advance(xa[0], xa[1], xa[2], fcst_len)\n",
    "  xe[0], xe[1], xe[2] = lor.advance(xt[0], xt[1], xt[2], fcst_len)\n",
    "  xferr[t,:] = xf[:] - xe[:]\n",
    "\n",
    "print('Analysis Error: ',np.sqrt(sum(sum(xaerr[:,:] * xaerr[:,:])) / float(nassim*3)))\n",
    "print('Background Error: ',np.sqrt(sum(sum(xberr[:,:] * xberr[:,:])) / float(nassim*3)))\n",
    "print('Forecast Error: ',np.sqrt(sum(sum(xferr[:,:] * xferr[:,:])) / float(nassim*3)))\n",
    "\n",
    "time2 = time.time()\n",
    "\n",
    "print(\"Total Time:\",time2-time1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atoc5060",
   "language": "python",
   "name": "atoc5060"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
